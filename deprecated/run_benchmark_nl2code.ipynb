{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"]     = os.getenv(\"WANDB_CASPIA_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]    = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]      = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]          = os.getenv(\"HF_CASPIA_API_KEY\")\n",
    "os.environ['TOGETHER_API_KEY']  = os.getenv('TOGETHER_CASPIA_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_CASPIA_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('caspia-technologies/benchmarking_rtl_svx')\n",
    "df_train = data['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "import json \n",
    "\n",
    "random_example = json.loads(df_train['parsed_description'][0])\n",
    "random_example_code = df_train['code'][0]\n",
    "pprint.pprint(random_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform this to a dataframe with 6 columns, 2 for each block, one for the system and one for the prompt\n",
    "df_train_parsed = pd.DataFrame()\n",
    "df_train_parsed['system_block_summary'] = df_train['parsed_description'].apply(lambda x: json.loads(x)['block_summary']['system'])\n",
    "df_train_parsed['prompt_block_summary'] = df_train['parsed_description'].apply(lambda x: json.loads(x)['block_summary']['prompt'])\n",
    "df_train_parsed['system_detailed_global_summary'] = df_train['parsed_description'].apply(lambda x: json.loads(x)['detailed_global_summary']['system'])\n",
    "df_train_parsed['prompt_detailed_global_summary'] = df_train['parsed_description'].apply(lambda x: json.loads(x)['detailed_global_summary']['prompt'])\n",
    "df_train_parsed['system_high_level_global_summary'] = df_train['parsed_description'].apply(lambda x: json.loads(x)['high_level_global_summary']['system'])\n",
    "df_train_parsed['prompt_high_level_global_summary'] = df_train['parsed_description'].apply(lambda x: json.loads(x)['high_level_global_summary']['prompt'])\n",
    "df_train_parsed['code'] = df_train['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_parsed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Inference\n",
    "\n",
    "Model List:\n",
    "- Together: meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n",
    "- Huggingface: huggingface:Qwen/Qwen2.5-72B-Instruct\n",
    "- Anthropic: anthropic:claude-3-5-sonnet-20240620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it for the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(index):\n",
    "    first_example = json.loads(df_train['parsed_description'][index])\n",
    "    first_code = df_train['code'][index]\n",
    "    return first_example, first_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Suite Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import weave\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Helper function to count tokens for a given text\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o')\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "class LLMRequestError(Exception):\n",
    "    pass\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('SVx')\n",
    "\n",
    "@weave.op()\n",
    "def track_token_metrics(messages, response):\n",
    "    \"\"\"Track token usage and other metrics for the LLM call\"\"\"\n",
    "    # Count input tokens\n",
    "    input_tokens = sum(count_tokens(message['content']) for message in messages)\n",
    "    \n",
    "    # Count output tokens\n",
    "    output_text = response.choices[0].message.content\n",
    "    output_tokens = count_tokens(output_text)\n",
    "    \n",
    "    return {\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'total_tokens': input_tokens + output_tokens,\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10), #exponential backoff\n",
    "    retry=retry_if_exception_type(Exception),\n",
    "    before_sleep=lambda retry_state: print(f\"Attempt {retry_state.attempt_number} failed. Retrying...\")\n",
    ")\n",
    "def make_llm_request(client, model, messages):\n",
    "    \"\"\"Make LLM request with automatic input/output tracking\"\"\"\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.01\n",
    "        )\n",
    "        \n",
    "        # Track metrics\n",
    "        metrics = track_token_metrics(messages, response)\n",
    "        \n",
    "        return {\n",
    "            'response': response.choices[0].message.content,\n",
    "            'metrics': metrics,\n",
    "            'provider': 'together' if 'together' in str(client) else 'aisuite',\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in make_llm_request for model {model}: {str(e)}\")\n",
    "        raise LLMRequestError(f\"Failed to generate response: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai \n",
    "from together import Together \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ask_llm(query, query_code):\n",
    "    #These are the examples for 2-shot learning, don't use these examples for inference or result benchmarking \n",
    "    example1, example_code1 = get_example(1)\n",
    "    example2, example_code2 = get_example(1234)\n",
    "\n",
    "    client = ai.Client()\n",
    "\n",
    "    models = {\n",
    "        \"aisuite\": {\n",
    "            \"client\": ai.Client(),\n",
    "            \"models\": {\n",
    "                \"Claude-3.5-Sonnet\": \"anthropic:claude-3-5-sonnet-20240620\"\n",
    "            }\n",
    "        },\n",
    "        \"together\": {\n",
    "            \"client\": Together(),\n",
    "            \"models\": {\n",
    "                \"Llama-3.3-70B-It\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "                \"Mistral-Small-3-24B-It\": \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "            }\n",
    "        }    \n",
    "    }\n",
    "\n",
    "    system_message = \"\"\"You are an helpful assistant. You will be given a query along with two example queries and corresponding answers to follow.\n",
    "        Based on that, you will need to provide a solution to the query. Only answer with Verilog code block, do not generate anything else. Follow the example format for output.\n",
    "        \\n\\n <Example 1> ##Instruction: {0} ##Query: {1} ##Answer: {2} \\n\\n <\\Example 1> \\n\\n <Example 2>\n",
    "        ##Instruction: {3} ##Query: {4} ##Answer: {5} \\n\\n <\\Example 2> \\n\\n <User Query> \\n\\n\"\"\"\n",
    "\n",
    "    formatted_system_message = system_message.format(\n",
    "        example1['block_summary']['system'],\n",
    "        example1['block_summary']['prompt'],\n",
    "        example_code1,\n",
    "        example2['block_summary']['system'],\n",
    "        example2['block_summary']['prompt'],\n",
    "        example_code2\n",
    "    )\n",
    "\n",
    "    user_message = f\"{query['block_summary']['system']} {query['block_summary']['prompt']} \\n\\n </User Query>\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": formatted_system_message\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response_dataframe = pd.DataFrame()\n",
    "\n",
    "    # write for loop for traversing each client and model and generate response only for model_names \n",
    "    for client_name, client_data in models.items():\n",
    "        print(f\"Accessing {client_name} client...\")\n",
    "        for model_name, model in client_data['models'].items():\n",
    "            print(f\"Attempting response from {model_name}...\")\n",
    "            try:\n",
    "                llm_response = make_llm_request(client_data['client'], model, messages)\n",
    "                response_dataframe[f\"{client_name}_{model_name}\"] = [llm_response['response']] #[response.choices[0].message.content]\n",
    "                print(f\"Response from {model_name} generated successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error after all retries for {model_name}: {e}\")\n",
    "                response_dataframe[f\"{client_name}_{model_name}\"] = [None]\n",
    "    \n",
    "    #response_dataframe['ground_truth'] = query_code\n",
    "    #insert in first column \n",
    "    response_dataframe.insert(0, 'ground_truth', query_code)\n",
    "\n",
    "    return response_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in Jupyter notebook:\n",
    "\n",
    "# Then run your query:\n",
    "query, query_code = get_example(2)\n",
    "\n",
    "# Execute the async function\n",
    "results = ask_llm(query, query_code)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame with same columns as response\n",
    "df_result = pd.DataFrame(columns=['ground_truth'])\n",
    "\n",
    "# Process dataframe rows\n",
    "for index, row in df_train_parsed.iterrows():\n",
    "    try:\n",
    "        # Get query and code\n",
    "        query, query_code = get_example(index)\n",
    "        \n",
    "        # Get LLM response\n",
    "        response_df = ask_llm(query, query_code)\n",
    "        \n",
    "        # Concatenate new response with existing results\n",
    "        df_result = pd.concat([df_result, response_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Processed row {index+1}\")\n",
    "        \n",
    "        if index == 2:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Display results\n",
    "print(f\"Total rows processed: {len(df_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
