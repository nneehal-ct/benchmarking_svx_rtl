{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook judges generated Verilog code against ground truth code and assigns scores to several aspects of the generated code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required libraries if needed\n",
    "# %pip install openai pandas wandb weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import OpenAIError, RateLimitError\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import weave\n",
    "import math\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TOGETHER_API_KEY=os.getenv(\"TOGETHER_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "SAMBANOVA_API_KEY= os.getenv(\"SAMBANOVA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def openai_response(prompt):\n",
    "#     # client = OpenAI()\n",
    "\n",
    "#     # Initialize Weave Tracing\n",
    "#     # weave.init('SVx')\n",
    "\n",
    "#     #test_sambanova_api()\n",
    "#     client = OpenAI(\n",
    "#         api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "#         base_url=\"https://api.sambanova.ai/v1\",\n",
    "#     )\n",
    "\n",
    "#     response = client.chat.completions.create(\n",
    "#             model=\"Llama-3.1-Tulu-3-405B\",\n",
    "#             messages=[{\"role\": \"system\", \"content\": \"You are an expert in Verilog hardware verification.\"},\n",
    "#                       {\"role\": \"user\", \"content\": prompt}],\n",
    "#             temperature=0\n",
    "#         )    \n",
    "#     # print(response)\n",
    "#     # response = client.chat.completions.create(\n",
    "#     #         model=\"gpt-4o\",\n",
    "#     #         messages=[{\"role\": \"system\", \"content\": \"You are an expert in Verilog hardware verification.\"},\n",
    "#     #                   {\"role\": \"user\", \"content\": prompt}],\n",
    "#     #         temperature=0\n",
    "#     #     )    \n",
    "#     # result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "#     result = response.choices[0].message.content\n",
    "#     # print(result)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_response(prompt):\n",
    "    # client = OpenAI(\n",
    "    #     api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    #     base_url=\"https://api.sambanova.ai/v1\",\n",
    "    # )\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Initialize Weave Tracing\n",
    "    weave.init('SVx')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # response = client.chat.completions.create(\n",
    "            #     model=\"Llama-3.1-Tulu-3-405B\",\n",
    "            #     messages=[{\"role\": \"system\", \"content\": \"You are an expert in Verilog hardware verification.\"},\n",
    "            #               {\"role\": \"user\", \"content\": prompt}],\n",
    "            #     temperature=0\n",
    "            # )\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an expert in Verilog hardware verification.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )  \n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            return result\n",
    "\n",
    "        except RateLimitError:\n",
    "            print(\"Rate limit exceeded. Retrying after a short delay...\")\n",
    "            time.sleep(10)  # Wait 10 seconds before retrying\n",
    "\n",
    "        except OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}. Retrying...\")\n",
    "            time.sleep(5)  # A general delay in case of other API errors\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break  # Exit the loop if there is an unexpected error\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LaaJ Evaluation metrics:\n",
    "- Logical Equivalence\n",
    "- Signal Behavior\n",
    "- Edge Case Handling\n",
    "- Code Modularity\n",
    "- Resource Efficiency\n",
    "- Timing and Pipeline Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # Regular expression to find JSON-like structures\n",
    "    json_pattern = re.compile(r'\\{.*?\\}', re.DOTALL)\n",
    "    \n",
    "    matches = json_pattern.findall(text)\n",
    "    \n",
    "    for match in matches:\n",
    "        try:\n",
    "            parsed_json = json.loads(match)\n",
    "            return parsed_json  # Return the first successfully parsed JSON\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_verilog_code(ground_truth, generated_code):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the following Verilog/SystemVerilog code against the ground truth based on these criteria:\n",
    "    \n",
    "    **Ground Truth Code:**\n",
    "    ```verilog\n",
    "    {ground_truth}\n",
    "    ```\n",
    "\n",
    "    **Generated Code:**\n",
    "    ```verilog\n",
    "    {generated_code}\n",
    "    ```\n",
    "    The evaluation criteria are as follows:\n",
    "    - **Logical Equivalence** (0-5) : \n",
    "        - Think about the below points before making the output JSON object\n",
    "        - Do both implementations produce identical outputs for all inputs?\n",
    "        - Identify any functional differences.\n",
    "        - Assign an integer **score (0-5)** value based on how closely the generated code matches the ground truth.\n",
    "        \n",
    "    - **Signal Behavior** (0-5):\n",
    "        - Think about the below points before making the output JSON object\n",
    "        - Do the registers, wires, and combinational logic behave the same way in both versions?\n",
    "        - Are state transitions in FSMs identical?\n",
    "        - Are signal updates happening at the correct time?\n",
    "        - Assign an integer **score (0-5)** value based on how accurately the generated code preserves signal behavior.\n",
    "\n",
    "    - **Edge Case Handling** (0-5):\n",
    "        - Think about the below points before making the output JSON object\n",
    "        - Does the generated code correctly implement synchronous and asynchronous resets?\n",
    "        - Are there any potential issues with metastability, clock domain transfers, or data integrity?\n",
    "        - Are unexpected behaviors prevented under corner cases?\n",
    "        - Assign an integer **score (0-5)** value accordingly.\n",
    "\n",
    "    - **Code Modularity** (0-5):\n",
    "        - Think about the below points before making the output JSON object\n",
    "        - Does the generated code follow modular design principles?\n",
    "        - Are parameters, functions, and submodules effectively used?\n",
    "        - Are unnecessary dependencies avoided?\n",
    "        - Assign an integer **score (0-5)** value accordingly.\n",
    "\n",
    "    - **Resource Efficiency** (0-5):\n",
    "        - Think about the below points before making the output JSON object\n",
    "        - Does the generated code use an excessive number of registers, combinational logic, or memory?\n",
    "        - Are there inefficient resource utilization patterns?\n",
    "        - Are logic and memory components minimized while maintaining functionality?\n",
    "        - Assign an integer **score (0-5)** value accordingly.\n",
    "\n",
    "\n",
    "    - **Timing & Pipeline Depth** (0-5):\n",
    "        - Does the generated code introduce unnecessary delays?\n",
    "        - Does it increase the critical path?\n",
    "        - Are pipeline stages maintained or improved?\n",
    "        - Assign an integer **score (0-5)** value.\n",
    "\n",
    "\n",
    "    Return the scores exactly in the following **JSON format**:\n",
    "    ```\n",
    "    {{\n",
    "      \"logical_equivalence\": X,\n",
    "      \"signal_behavior\": X,\n",
    "      \"edge_case_handling\": X,\n",
    "      \"code_modularity\": X,\n",
    "      \"resource_efficiency\": X,\n",
    "      \"timing_pipeline_depth\": X\n",
    "    }}\n",
    "    ```\n",
    "    For the scores, only mention the integer score value (0-5) and nothing else.\n",
    "    Only return the score in JSON format. No need to explain anythings else. Do not include any other text or information in the response.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = openai_response(prompt)\n",
    "\n",
    "        scores = extract_json(result)\n",
    "        \n",
    "        # Convert JSON response into Python dictionary\n",
    "        # scores = json.loads(parsed_result)\n",
    "        # print(scores)\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error\", str(e)\n",
    "    \n",
    "    # result = {}\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# def evaluate_logical_equivalence(ground_truth, generated_code):\n",
    "#     \"\"\"Uses an LLM to evaluate logical equivalence between ground truth and generated Verilog code.\"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     Compare the logical equivalence of the following Verilog/SystemVerilog codes. \n",
    "    \n",
    "#     **Ground Truth Code:**\n",
    "#     ```verilog\n",
    "#     {ground_truth}\n",
    "#     ```\n",
    "\n",
    "#     **Generated Code:**\n",
    "#     ```verilog\n",
    "#     {generated_code}\n",
    "#     ```\n",
    "\n",
    "#     - Do both implementations produce identical outputs for all inputs?\n",
    "#     - Identify any functional differences.\n",
    "#     - Assign a **score (0-5)** based on how closely the generated code matches the ground truth.\n",
    "\n",
    "#     Format the response as:\n",
    "#     ```\n",
    "#     Score: X\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "         \n",
    "#         result = openai_response(prompt)\n",
    "        \n",
    "#         # Extract score and explanation\n",
    "#         lines = result.split(\"\\n\")\n",
    "#         score = next((line.split(\":\")[1].strip() for line in lines if \"Score:\" in line), \"N/A\")        \n",
    "#         return int(score) if score.isdigit() else \"N/A\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return \"Error\", str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_signal_behavior(ground_truth, generated_code):\n",
    "#     \"\"\"Uses an LLM to evaluate signal behavior differences between ground truth and generated Verilog code.\"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     Analyze the signal behavior of the following Verilog/SystemVerilog codes. \n",
    "    \n",
    "#     **Ground Truth Code:**\n",
    "#     ```verilog\n",
    "#     {ground_truth}\n",
    "#     ```\n",
    "\n",
    "#     **Generated Code:**\n",
    "#     ```verilog\n",
    "#     {generated_code}\n",
    "#     ```\n",
    "\n",
    "#     - Do the registers, wires, and combinational logic behave the same way in both versions?\n",
    "#     - Are state transitions in FSMs identical?\n",
    "#     - Are signal updates happening at the correct time?\n",
    "#     - Assign a **score (0-5)** based on how accurately the generated code preserves signal behavior.\n",
    "\n",
    "#     Format the response as:\n",
    "#     ```\n",
    "#     Score: X\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     try:\n",
    "        \n",
    "#         result = openai_response(prompt)\n",
    "        \n",
    "#         # Extract score and explanation\n",
    "#         lines = result.split(\"\\n\")\n",
    "#         score = next((line.split(\":\")[1].strip() for line in lines if \"Score:\" in line), \"N/A\")        \n",
    "#         return int(score) if score.isdigit() else \"N/A\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return \"Error\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_edge_case_handling(ground_truth, generated_code):\n",
    "#     \"\"\"Uses an LLM to evaluate edge case handling in Verilog code.\"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     Evaluate whether the generated Verilog code correctly handles reset conditions, clock domain crossings, \n",
    "#     and asynchronous behavior compared to the ground truth.\n",
    "\n",
    "#     **Ground Truth Code:**\n",
    "#     ```verilog\n",
    "#     {ground_truth}\n",
    "#     ```\n",
    "\n",
    "#     **Generated Code:**\n",
    "#     ```verilog\n",
    "#     {generated_code}\n",
    "#     ```\n",
    "\n",
    "#     - Does the generated code correctly implement synchronous and asynchronous resets?\n",
    "#     - Are there any potential issues with metastability, clock domain transfers, or data integrity?\n",
    "#     - Are unexpected behaviors prevented under corner cases?\n",
    "#     - Assign a **score (0-5)**.\n",
    "\n",
    "#     Format the response as:\n",
    "#     ```\n",
    "#     Score: X\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         result = openai_response(prompt)\n",
    "        \n",
    "#         # Extract score and explanation\n",
    "#         lines = result.split(\"\\n\")\n",
    "#         score = next((line.split(\":\")[1].strip() for line in lines if \"Score:\" in line), \"N/A\")        \n",
    "#         return int(score) if score.isdigit() else \"N/A\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return \"Error\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_code_modularity(ground_truth, generated_code):\n",
    "#     \"\"\"Evaluates code modularity between ground truth and generated Verilog code.\"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     Compare the modularity of the following Verilog/SystemVerilog codes. \n",
    "    \n",
    "#     **Ground Truth Code:**\n",
    "#     ```verilog\n",
    "#     {ground_truth}\n",
    "#     ```\n",
    "\n",
    "#     **Generated Code:**\n",
    "#     ```verilog\n",
    "#     {generated_code}\n",
    "#     ```\n",
    "\n",
    "#     - Does the generated code follow modular design principles?\n",
    "#     - Are parameters, functions, and submodules effectively used?\n",
    "#     - Are unnecessary dependencies avoided?\n",
    "#     - Assign a **score (0-5)** .\n",
    "\n",
    "#     Format the response as:\n",
    "#     ```\n",
    "#     Score: X\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         result = openai_response(prompt)\n",
    "        \n",
    "#         # Extract score and explanation\n",
    "#         lines = result.split(\"\\n\")\n",
    "#         score = next((line.split(\":\")[1].strip() for line in lines if \"Score:\" in line), \"N/A\")        \n",
    "#         return int(score) if score.isdigit() else \"N/A\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return \"Error\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_resource_efficiency(ground_truth, generated_code):\n",
    "#     \"\"\"Evaluates resource efficiency between ground truth and generated Verilog code.\"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     Compare the resource efficiency of the following Verilog/SystemVerilog codes. \n",
    "    \n",
    "#     **Ground Truth Code:**\n",
    "#     ```verilog\n",
    "#     {ground_truth}\n",
    "#     ```\n",
    "\n",
    "#     **Generated Code:**\n",
    "#     ```verilog\n",
    "#     {generated_code}\n",
    "#     ```\n",
    "\n",
    "#     - Does the generated code use an excessive number of registers, combinational logic, or memory?\n",
    "#     - Are there inefficient resource utilization patterns?\n",
    "#     - Are logic and memory components minimized while maintaining functionality?\n",
    "#     - Assign a **score (0-5)** .\n",
    "\n",
    "#     Format the response as:\n",
    "#     ```\n",
    "#     Score: X\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         result = openai_response(prompt)\n",
    "        \n",
    "#         # Extract score and explanation\n",
    "#         lines = result.split(\"\\n\")\n",
    "#         score = next((line.split(\":\")[1].strip() for line in lines if \"Score:\" in line), \"N/A\")\n",
    "        \n",
    "#         return int(score) if score.isdigit() else \"N/A\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return \"Error\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_timing_pipeline_depth(ground_truth, generated_code):\n",
    "#     \"\"\"Evaluates timing and pipeline depth between ground truth and generated Verilog code.\"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     Compare the timing and pipeline depth of the following Verilog/SystemVerilog codes. \n",
    "    \n",
    "#     **Ground Truth Code:**\n",
    "#     ```verilog\n",
    "#     {ground_truth}\n",
    "#     ```\n",
    "\n",
    "#     **Generated Code:**\n",
    "#     ```verilog\n",
    "#     {generated_code}\n",
    "#     ```\n",
    "\n",
    "#     - Does the generated code introduce unnecessary delays?\n",
    "#     - Does it increase the critical path?\n",
    "#     - Are pipeline stages maintained or improved?\n",
    "#     - Assign a **score (0-5)** .\n",
    "\n",
    "#     Format the response as:\n",
    "#     ```\n",
    "#     Score: X\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         result = openai_response(prompt)\n",
    "        \n",
    "#         # Extract score and explanation\n",
    "#         lines = result.split(\"\\n\")\n",
    "#         score = next((line.split(\":\")[1].strip() for line in lines if \"Score:\" in line), \"N/A\")\n",
    "        \n",
    "#         return int(score) if score.isdigit() else \"N/A\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return \"Error\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scaled_score(logical_equivalence, signal_behavior, edge_case_handling,\n",
    "                         code_modularity, resource_efficiency, timing_pipeline_depth):\n",
    "    weights = {\n",
    "        \"Logical_Score\": 0.50,\n",
    "        \"Signal_Score\": 0.10,\n",
    "        \"EdgeCase_Score\": 0.10,\n",
    "        \"Modularity_Score\": 0.10,\n",
    "        \"ResourceEfficiency_Score\": 0.10,\n",
    "        \"Timing_Score\": 0.10\n",
    "    }\n",
    "    \n",
    "    # Compute weighted sum\n",
    "    weighted_sum = (\n",
    "        logical_equivalence * weights[\"Logical_Score\"] +\n",
    "        signal_behavior * weights[\"Signal_Score\"] +\n",
    "        edge_case_handling * weights[\"EdgeCase_Score\"] +\n",
    "        code_modularity * weights[\"Modularity_Score\"] +\n",
    "        resource_efficiency * weights[\"ResourceEfficiency_Score\"] +\n",
    "        timing_pipeline_depth * weights[\"Timing_Score\"]\n",
    "    )\n",
    "    \n",
    "    # Maximum possible score with given weights\n",
    "    max_score = 5.0  # Each variable has a max value of 5\n",
    "    max_weighted_sum = sum(weight * max_score for weight in weights.values())\n",
    "    \n",
    "    # Scale the weighted sum to be between 0.0 and 1.0\n",
    "    scaled_score = weighted_sum / max_weighted_sum\n",
    "\n",
    "    # Ensure two decimal places using ceiling function\n",
    "    scaled_score = math.ceil(scaled_score * 100) / 100.0\n",
    "    \n",
    "    return scaled_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(ground_truth, generated_code):\n",
    "\n",
    "    # logical_equivalence = evaluate_logical_equivalence(ground_truth, generated_code)\n",
    "    # signal_behavior = evaluate_signal_behavior(ground_truth, generated_code)\n",
    "    # edge_case_handling = evaluate_edge_case_handling(ground_truth, generated_code)\n",
    "    # code_modularity = evaluate_code_modularity(ground_truth, generated_code)\n",
    "    # resource_efficiency = evaluate_resource_efficiency(ground_truth, generated_code)\n",
    "    # timing_pipeline_depth = evaluate_timing_pipeline_depth(ground_truth, generated_code)\n",
    "\n",
    "    scores = evaluate_verilog_code(ground_truth, generated_code)\n",
    "    print(scores)\n",
    "\n",
    "    logical_equivalence = scores[\"logical_equivalence\"]\n",
    "    signal_behavior = scores[\"signal_behavior\"]\n",
    "    edge_case_handling = scores[\"edge_case_handling\"]\n",
    "    code_modularity = scores[\"code_modularity\"]\n",
    "    resource_efficiency = scores[\"resource_efficiency\"] \n",
    "    timing_pipeline_depth = scores[\"timing_pipeline_depth\"]\n",
    "\n",
    "    # logical_equivalence = scores.logical_equivalence\n",
    "    # signal_behavior = scores.signal_behavior\n",
    "    # edge_case_handling = scores.edge_case_handling\n",
    "    # code_modularity = scores.code_modularity\n",
    "    # resource_efficiency = scores.resource_efficiency\n",
    "    # timing_pipeline_depth = scores.timing_pipeline_depth\n",
    "    \n",
    "    scaled_score = compute_scaled_score(logical_equivalence, signal_behavior, edge_case_handling,\n",
    "                         code_modularity, resource_efficiency, timing_pipeline_depth)\n",
    "    \n",
    "    score_json = {\n",
    "        \"scaled_score\":scaled_score,\n",
    "        \"logical_equivalence\":logical_equivalence,\n",
    "        \"signal_behavior\":signal_behavior,\n",
    "        \"edge_case_handling\":edge_case_handling,\n",
    "        \"code_modularity\":code_modularity,\n",
    "        \"resource_efficiency\":resource_efficiency,\n",
    "        \"timing_pipeline_depth\":timing_pipeline_depth\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    return json.dumps(score_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_column(df_source, df_target, col1, col2, new_col_name):\n",
    "    \"\"\"\n",
    "    Takes df_source, extracts values from col1 and col2, processes them with make_json,\n",
    "    and adds a new column to df_target.\n",
    "\n",
    "    Args:\n",
    "        df_source (pd.DataFrame): Source dataframe with input columns. -> benchmark dataset \n",
    "        col1 (str): Name of the first column in df_source. -> ground truth\n",
    "        col2 (str): Name of the second column in df_source. -> generated code (for a specific LLM)\n",
    "        df_target (pd.DataFrame): Target dataframe to add the new column. -> Laaj scores dataset\n",
    "        new_col_name (str): Name of the new column in df_target. -> specific LLM name\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated df_target with the new column.\n",
    "    \"\"\"\n",
    "\n",
    "    df_target[new_col_name] = df_source.apply(lambda row: make_json(row[col1], row[col2]), axis=1).to_list()\n",
    "\n",
    "\n",
    "    return df_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>aisuite_Qwen-2.5-72B-It</th>\n",
       "      <th>aisuite_Claude-3.5-Sonnet</th>\n",
       "      <th>together_Llama-3.3-70B-It</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n assign enabled = enable; \\n dmac_data_move...</td>\n",
       "      <td>```verilog\\nreg [ID_WIDTH-1:0] response_id_reg...</td>\n",
       "      <td>reg [2:0] state;\\nreg [ID_WIDTH-1:0] active_id...</td>\n",
       "      <td>always @ (posedge s_axis_aclk) \\nbegin \\n    i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n wire or_1, or_2; \\n wire [W-2:0] zero_comp...</td>\n",
       "      <td>wire or_1, or_2;\\nwire [W-2:0] zero_comp;\\nwir...</td>\n",
       "      <td>wire or_1, or_2;\\nwire [W-2:0] zero_comp;\\nwir...</td>\n",
       "      <td>wire or_1, or_2; \\nwire [W-2:0] zero_comp; \\nw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n input clk,rst; \\n input rd_en; \\n input wr...</td>\n",
       "      <td>```verilog\\nparameter MAX_COUNT = 15;\\nreg [3:...</td>\n",
       "      <td>reg full, emptyp;\\nreg [3:0] count;\\nparameter...</td>\n",
       "      <td>reg [3:0] count;\\nreg fullp, emptyp;\\nalways @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n  reg rxd_buf; \\n  reg clk_buf; \\n  reg [7:...</td>\n",
       "      <td>```verilog\\nreg rxd_buf;\\nreg clk_buf;\\nreg [7...</td>\n",
       "      <td>reg rxd_buf;\\nreg clk_buf;\\nreg [7:0] data_buf...</td>\n",
       "      <td>reg [7:0] rxd_buf;\\nreg clk_buf;\\nreg [7:0] da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ground_truth  \\\n",
       "0   \\n assign enabled = enable; \\n dmac_data_move...   \n",
       "1   \\n wire or_1, or_2; \\n wire [W-2:0] zero_comp...   \n",
       "2   \\n input clk,rst; \\n input rd_en; \\n input wr...   \n",
       "3   \\n  reg rxd_buf; \\n  reg clk_buf; \\n  reg [7:...   \n",
       "\n",
       "                             aisuite_Qwen-2.5-72B-It  \\\n",
       "0  ```verilog\\nreg [ID_WIDTH-1:0] response_id_reg...   \n",
       "1  wire or_1, or_2;\\nwire [W-2:0] zero_comp;\\nwir...   \n",
       "2  ```verilog\\nparameter MAX_COUNT = 15;\\nreg [3:...   \n",
       "3  ```verilog\\nreg rxd_buf;\\nreg clk_buf;\\nreg [7...   \n",
       "\n",
       "                           aisuite_Claude-3.5-Sonnet  \\\n",
       "0  reg [2:0] state;\\nreg [ID_WIDTH-1:0] active_id...   \n",
       "1  wire or_1, or_2;\\nwire [W-2:0] zero_comp;\\nwir...   \n",
       "2  reg full, emptyp;\\nreg [3:0] count;\\nparameter...   \n",
       "3  reg rxd_buf;\\nreg clk_buf;\\nreg [7:0] data_buf...   \n",
       "\n",
       "                           together_Llama-3.3-70B-It  \n",
       "0  always @ (posedge s_axis_aclk) \\nbegin \\n    i...  \n",
       "1  wire or_1, or_2; \\nwire [W-2:0] zero_comp; \\nw...  \n",
       "2  reg [3:0] count;\\nreg fullp, emptyp;\\nalways @...  \n",
       "3  reg [7:0] rxd_buf;\\nreg clk_buf;\\nreg [7:0] da...  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/result.csv\")\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logical_equivalence': 1, 'signal_behavior': 2, 'edge_case_handling': 3, 'code_modularity': 2, 'resource_efficiency': 3, 'timing_pipeline_depth': 2}\n",
      "{'logical_equivalence': 5, 'signal_behavior': 5, 'edge_case_handling': 5, 'code_modularity': 5, 'resource_efficiency': 5, 'timing_pipeline_depth': 5}\n",
      "{'logical_equivalence': 4, 'signal_behavior': 4, 'edge_case_handling': 5, 'code_modularity': 4, 'resource_efficiency': 4, 'timing_pipeline_depth': 5}\n",
      "{'logical_equivalence': 2, 'signal_behavior': 3, 'edge_case_handling': 3, 'code_modularity': 4, 'resource_efficiency': 4, 'timing_pipeline_depth': 3}\n",
      "{'logical_equivalence': 1, 'signal_behavior': 2, 'edge_case_handling': 3, 'code_modularity': 2, 'resource_efficiency': 2, 'timing_pipeline_depth': 2}\n",
      "{'logical_equivalence': 5, 'signal_behavior': 5, 'edge_case_handling': 5, 'code_modularity': 5, 'resource_efficiency': 5, 'timing_pipeline_depth': 5}\n",
      "{'logical_equivalence': 3, 'signal_behavior': 3, 'edge_case_handling': 4, 'code_modularity': 4, 'resource_efficiency': 4, 'timing_pipeline_depth': 4}\n",
      "{'logical_equivalence': 2, 'signal_behavior': 2, 'edge_case_handling': 2, 'code_modularity': 3, 'resource_efficiency': 3, 'timing_pipeline_depth': 2}\n",
      "{'logical_equivalence': 3, 'signal_behavior': 4, 'edge_case_handling': 4, 'code_modularity': 2, 'resource_efficiency': 3, 'timing_pipeline_depth': 3}\n",
      "{'logical_equivalence': 5, 'signal_behavior': 5, 'edge_case_handling': 5, 'code_modularity': 5, 'resource_efficiency': 5, 'timing_pipeline_depth': 5}\n",
      "Rate limit exceeded. Retrying after a short delay...\n",
      "Rate limit exceeded. Retrying after a short delay...\n",
      "Rate limit exceeded. Retrying after a short delay...\n",
      "Rate limit exceeded. Retrying after a short delay...\n",
      "{'logical_equivalence': 4, 'signal_behavior': 5, 'edge_case_handling': 5, 'code_modularity': 5, 'resource_efficiency': 5, 'timing_pipeline_depth': 5}\n",
      "{'logical_equivalence': 3, 'signal_behavior': 2, 'edge_case_handling': 3, 'code_modularity': 4, 'resource_efficiency': 4, 'timing_pipeline_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame()\n",
    "output_df = add_score_column(df, output_df, \"ground_truth\", \"together_Llama-3.3-70B-It\",\"together_Llama-3.3-70B-It\")\n",
    "output_df = add_score_column(df, output_df, \"ground_truth\", \"aisuite_Qwen-2.5-72B-It\",\"aisuite_Qwen-2.5-72B-It\")\n",
    "output_df = add_score_column(df, output_df, \"ground_truth\", \"aisuite_Claude-3.5-Sonnet\",\"aisuite_Claude-3.5-Sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>together_Llama-3.3-70B-It</th>\n",
       "      <th>aisuite_Qwen-2.5-72B-It</th>\n",
       "      <th>aisuite_Claude-3.5-Sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"scaled_score\": 0.34, \"logical_equivalence\": ...</td>\n",
       "      <td>{\"scaled_score\": 0.32, \"logical_equivalence\": ...</td>\n",
       "      <td>{\"scaled_score\": 0.62, \"logical_equivalence\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"scaled_score\": 1.0, \"logical_equivalence\": 5...</td>\n",
       "      <td>{\"scaled_score\": 1.0, \"logical_equivalence\": 5...</td>\n",
       "      <td>{\"scaled_score\": 1.0, \"logical_equivalence\": 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"scaled_score\": 0.84, \"logical_equivalence\": ...</td>\n",
       "      <td>{\"scaled_score\": 0.68, \"logical_equivalence\": ...</td>\n",
       "      <td>{\"scaled_score\": 0.9, \"logical_equivalence\": 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"scaled_score\": 0.54, \"logical_equivalence\": ...</td>\n",
       "      <td>{\"scaled_score\": 0.45, \"logical_equivalence\": ...</td>\n",
       "      <td>{\"scaled_score\": 0.6, \"logical_equivalence\": 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           together_Llama-3.3-70B-It  \\\n",
       "0  {\"scaled_score\": 0.34, \"logical_equivalence\": ...   \n",
       "1  {\"scaled_score\": 1.0, \"logical_equivalence\": 5...   \n",
       "2  {\"scaled_score\": 0.84, \"logical_equivalence\": ...   \n",
       "3  {\"scaled_score\": 0.54, \"logical_equivalence\": ...   \n",
       "\n",
       "                             aisuite_Qwen-2.5-72B-It  \\\n",
       "0  {\"scaled_score\": 0.32, \"logical_equivalence\": ...   \n",
       "1  {\"scaled_score\": 1.0, \"logical_equivalence\": 5...   \n",
       "2  {\"scaled_score\": 0.68, \"logical_equivalence\": ...   \n",
       "3  {\"scaled_score\": 0.45, \"logical_equivalence\": ...   \n",
       "\n",
       "                           aisuite_Claude-3.5-Sonnet  \n",
       "0  {\"scaled_score\": 0.62, \"logical_equivalence\": ...  \n",
       "1  {\"scaled_score\": 1.0, \"logical_equivalence\": 5...  \n",
       "2  {\"scaled_score\": 0.9, \"logical_equivalence\": 4...  \n",
       "3  {\"scaled_score\": 0.6, \"logical_equivalence\": 3...  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
